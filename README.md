# 3C-Assembly-Strategy-Based-on-DRL
#ABSTRACT
Addressing the issues of existing 3C assembly methods that rely on precise contact state models, low sampling efficiency, and poor safety, this paper proposes a research method for a manipulator-based 3C assembly strategy utilizing deep reinforcement learning. Initially, the study constructs a simulation task for 3C assembly involving a UR manipulator and flexible printed circuits (FPC) buckling within the MuJoCo development environment to mirror real-world assembly conditions. By incorporating a Gaussian distribution-based policy network suitable for continuous action spaces and employing the maximum entropy method to enhance the algorithm's exploratory capabilities, this study develops an efficient method for training autonomous assembly behavior strategies. We have successfully established a 3C assembly simulation environment that accurately simulates key physical parameters such as position, contact force, and torque, modeling the assembly task as a Markov decision process. Considering the semi-flexible nature of FPC, we control the magnitude of adaptive contact force to achieve compliant assembly of FPCs. Comprehensive simulation experiments demonstrate that the SAC algorithm proposed in this study enables the robot to autonomously and obediently complete the 3C assembly tasks, exhibiting good accuracy and stability. The assembly success rate reaches 93%, and after training with the reinforcement learning strategy, the contact force meets the preset range, achieving the effect of compliant assembly.
